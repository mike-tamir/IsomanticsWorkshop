{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Isomantics - SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt\n",
    "! pip install bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Manually set list of translations (embedding, lg1, lg2)\n",
    "    \n",
    "    svds = ['s','s1']\n",
    "    languages = ['en','ru','de','es','fr','it', 'zh-CN']\n",
    "    stats = ['min','max','mean','median','std','fro','acc']\n",
    "    \n",
    "    translations=[]\n",
    "    \n",
    "    for lang1 in languages:\n",
    "        for lang2 in languages:\n",
    "            translations.append(('fasttext_top',lang1, lang2))\n",
    "    \n",
    "    svd = {}\n",
    "    svd1 = {}\n",
    "    \n",
    "    for translation in translations:\n",
    "        embedding, lg1, lg2 = translation\n",
    "        # Vocab/Vectors/Dicts\n",
    "        lg1_vocab, lg1_vectors, lg2_vocab, lg2_vectors = \\\n",
    "            pickle_rw((lg1 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg1 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vocab', 0),\n",
    "                      (lg2 + '_' + embedding.split('_')[0] + '_vectors', 0),\n",
    "                      write=False)\n",
    "        lg1_dict = make_dict(lg1_vocab, lg1_vectors)\n",
    "        lg2_dict = make_dict(lg2_vocab, lg2_vectors)\n",
    "\n",
    "        print('Translation: '+lg1+'->'+lg2+'\\n')\n",
    "\n",
    "        # Train/Test Vocab/Vectors\n",
    "        vocab_train, vocab_test = vocab_train_test(embedding, lg1, lg2, lg1_vocab)\n",
    "        X_train, X_test, y_train, y_test = vectors_train_test(vocab_train,\n",
    "                                                              vocab_test,lg1_dict,lg2_dict)\n",
    " \n",
    "        \n",
    "        # Fit tranlation matrix to training data\n",
    "        model, history, T, tf,I, M, fro = translation_matrix(X_train, y_train)\n",
    "        \n",
    "        results_df = translation_results(X_test, y_test, vocab_test, T,\n",
    "                                         lg2_vectors, lg2_vocab)\n",
    "        acc = T_norm_EDA(results_df)\n",
    "        \n",
    "        U,s,Vh = SVD(T)\n",
    "        \n",
    "        s1 = log(s)\n",
    "    \n",
    "        \n",
    "        for stat in stats:\n",
    "            svd[stat,translation[1],translation[2]] = stat_calc(stat, s, fro, acc)\n",
    "            svd1[stat,translation[1],translation[2]] = stat_calc(stat, s1, fro, acc)\n",
    "            \n",
    "        \n",
    "    #Exporting DataFrames for SVD Heatmaps\n",
    "    \n",
    "    s_df = make_df(languages,languages)\n",
    "    s1_df = make_df(languages,languages)\n",
    "    \n",
    "\n",
    "    for stat in stats:\n",
    "        for lang1 in languages:\n",
    "            for lang2 in languages:\n",
    "                s_df.set_value(lang1,lang2,svd[stat,lang1,lang2])\n",
    "                s1_df.set_value(lang1,lang2,svd1[stat,lang1,lang2])\n",
    "\n",
    "        s_df.to_csv('../HeatmapData/T/s_{}.csv'.format(stat),columns = languages)\n",
    "        s1_df.to_csv('../HeatmapData/T/s1_{}.csv'.format(stat),columns = languages)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-263a07220104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 's_df' is not defined"
     ]
    }
   ],
   "source": [
    "s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
